{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e67ec2",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "In this notebook we demonstrates how to run the entire experimental pipeline (Hyperparameter Optimization, Final Training, and Evaluation) for a model variation that will be selected in a later cell. We achive this by calling the core functions defined in `hpo.py`, `final_training.py`, and `evaluation.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01861b06",
   "metadata": {},
   "source": [
    "### Selecting a model\n",
    "Due to time contraints we have the models in list that will be ran as batches. This will be changed later. For now please only select one of the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44334059",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETUP_IDS_TO_RUN = ['resnet_head_aug', 'resnet_mid_noaug', 'mobilenet_head_noaug']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35b1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETUP_IDS_TO_RUN = ['resnet_head_noaug', 'mobilenet_mid_aug']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e035afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETUP_IDS_TO_RUN = ['resnet_mid_aug', 'mobilenet_head_aug', 'mobilenet_mid_noaug']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5457ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resnet_head_noaug', 'mobilenet_mid_aug']\n"
     ]
    }
   ],
   "source": [
    "print(SETUP_IDS_TO_RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9597ee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Will run pipeline for Setups ID: ['resnet_head_noaug', 'mobilenet_mid_aug'] ###\n",
      "Using device: cuda\n",
      "Output directories ensured.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import time # For potential timing if desired, though scripts handle internal timing\n",
    "import sys\n",
    "sys.path.append('../scripts') \n",
    "\n",
    "# Import shared utilities and configurations \n",
    "import utils\n",
    "import config # Contains HPO_CONFIG_LIST, UNFREEZE_MAPS, output dirs, defaults\n",
    "\n",
    "# Import the core processing functions from our scripts\n",
    "from hpo import perform_hpo_for_setup\n",
    "from final_train import perform_final_training_for_setup\n",
    "from evaluate import perform_evaluation_for_setup\n",
    "\n",
    "# --- Define the Single Setup to Run ---\n",
    "print(f\"### Will run pipeline for Setups ID: {SETUP_IDS_TO_RUN} ###\")\n",
    "\n",
    "# --- Global Parameters for this Run (can override config defaults if needed) ---\n",
    "# These would typically come from argparse in the scripts, here we set them directly\n",
    "# Or, we can rely on defaults set within config.py and the script functions\n",
    "HPO_EPOCHS = config.DEFAULT_HPO_EPOCHS\n",
    "FINAL_TRAIN_EPOCHS = config.DEFAULT_FINAL_TRAIN_EPOCHS\n",
    "PATIENCE = config.DEFAULT_PATIENCE\n",
    "BATCH_SIZE = config.DEFAULT_BATCH_SIZE # Using one batch size for simplicity here, scripts might allow separate\n",
    "\n",
    "# --- Device Setup ---\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Ensure Output Directories Exist ---\n",
    "config.create_output_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75336872",
   "metadata": {},
   "source": [
    "## Stage 1: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8932051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== STAGE 1: Hyperparameter Optimization =====\n",
      "\n",
      "Running HPO for setup: resnet_head_noaug\n",
      "--- Starting HPO for Setup ID: resnet_head_noaug ---\n",
      "  Trial 1/12 with params: {'lr_backbone': 0.0001, 'lr_head': 0.005, 'weight_decay': 0.01}\n",
      "Downloading https://thor.robots.ox.ac.uk/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 16.8M/792M [00:03<02:45, 4.67MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m hpo_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the HPO function from run_hpo.py\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m best_hpo_params_found \u001b[38;5;241m=\u001b[39m perform_hpo_for_setup(\n\u001b[1;32m      9\u001b[0m     setup_id\u001b[38;5;241m=\u001b[39msetup_id,\n\u001b[1;32m     10\u001b[0m     hpo_grid\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mHPO_CONFIG_LIST,\n\u001b[1;32m     11\u001b[0m     num_epochs_hpo\u001b[38;5;241m=\u001b[39mHPO_EPOCHS,\n\u001b[1;32m     12\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     13\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# The function saves its outputs to files within config.HPO_DIR\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m hpo_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m HPO Stage for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetup_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m finished in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(hpo_end_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mhpo_start_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Dev/Math392/MATH392_Final_Project/notebooks/../scripts/hpo.py:45\u001b[0m, in \u001b[0;36mperform_hpo_for_setup\u001b[0;34m(setup_id, hpo_grid, num_epochs_hpo, batch_size, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Data setup\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_datasets(\n\u001b[1;32m     46\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainval\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     47\u001b[0m     augment_train\u001b[38;5;241m=\u001b[39maugment_str, \u001b[38;5;66;03m# Pass string 'aug' or 'noaug'\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     val_split_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;66;03m# Consistent validation split\u001b[39;00m\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     50\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_dataloaders(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     51\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_dataloaders(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset\u001b[38;5;241m=\u001b[39mval_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/Dev/Math392/MATH392_Final_Project/notebooks/../scripts/utils.py:108\u001b[0m, in \u001b[0;36mget_datasets\u001b[0;34m(task, root_dir, augment_train, val_split_ratio, random_seed)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainval\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Load the full trainval dataset *with non-augmented transforms* first\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     full_trainval_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mOxfordIIITPet(\n\u001b[1;32m    109\u001b[0m         root\u001b[38;5;241m=\u001b[39mroot_dir,\n\u001b[1;32m    110\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainval\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    111\u001b[0m         download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    112\u001b[0m         transform\u001b[38;5;241m=\u001b[39mval_test_transforms \u001b[38;5;66;03m# Use non-augmented for splitting reference\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Split the dataset indices\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     num_trainval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_trainval_dataset)\n",
      "File \u001b[0;32m~/miniconda3/envs/math392/lib/python3.11/site-packages/torchvision/datasets/oxford_iiit_pet.py:64\u001b[0m, in \u001b[0;36mOxfordIIITPet.__init__\u001b[0;34m(self, root, split, target_types, transforms, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_segs_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_anns_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrimaps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/math392/lib/python3.11/site-packages/torchvision/datasets/oxford_iiit_pet.py:132\u001b[0m, in \u001b[0;36mOxfordIIITPet._download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url, md5 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_RESOURCES:\n\u001b[0;32m--> 132\u001b[0m     download_and_extract_archive(url, download_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_folder), md5\u001b[38;5;241m=\u001b[39mmd5)\n",
      "File \u001b[0;32m~/miniconda3/envs/math392/lib/python3.11/site-packages/torchvision/datasets/utils.py:395\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    393\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 395\u001b[0m download_url(url, download_root, filename, md5)\n\u001b[1;32m    397\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/math392/lib/python3.11/site-packages/torchvision/datasets/utils.py:132\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[0;32m--> 132\u001b[0m     _urlretrieve(url, fpath)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/math392/lib/python3.11/site-packages/torchvision/datasets/utils.py:30\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m response\u001b[38;5;241m.\u001b[39mread(chunk_size):\n\u001b[1;32m     31\u001b[0m             fh\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m     32\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/miniconda3/envs/math392/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/math392/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/math392/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/math392/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*5} STAGE 1: Hyperparameter Optimization {'='*5}\")\n",
    "\n",
    "for setup_id in SETUP_IDS_TO_RUN:\n",
    "    print(f\"\\nRunning HPO for setup: {setup_id}\")\n",
    "    hpo_start_time = time.time()\n",
    "\n",
    "    # Call the HPO function from run_hpo.py\n",
    "    best_hpo_params_found = perform_hpo_for_setup(\n",
    "        setup_id=setup_id,\n",
    "        hpo_grid=config.HPO_CONFIG_LIST,\n",
    "        num_epochs_hpo=HPO_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=device\n",
    "        # The function saves its outputs to files within config.HPO_DIR\n",
    "    )\n",
    "\n",
    "    hpo_end_time = time.time()\n",
    "    print(f\"\\n{'*'*10} HPO Stage for {setup_id} finished in {(hpo_end_time - hpo_start_time)/60:.2f} minutes {'*'*10}\")\n",
    "\n",
    "    if best_hpo_params_found:\n",
    "        print(f\"Best HPO parameters found for {setup_id}: {best_hpo_params_found}\")\n",
    "        # These are also saved to results/hpo/best_params_{setup_id}.json\n",
    "    else:\n",
    "        print(f\"HPO did not complete successfully or find best parameters for {setup_id}. Cannot proceed.\")\n",
    "        # Consider raising an error or stopping the notebook here if HPO is critical for next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deedbae2",
   "metadata": {},
   "source": [
    "## Stage 2: Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "for setup_id in SETUP_IDS_TO_RUN:\n",
    "    if best_hpo_params_found:  # Proceed only if HPO was successful\n",
    "        print(f\"\\n{'='*20} STAGE 2: Final Model Training for {setup_id} {'='*20}\")\n",
    "        final_train_start_time = time.time()\n",
    "\n",
    "        # Call the final training function from run_final_training.py\n",
    "        # It will load the best_params file itself based on setup_id.\n",
    "        saved_model_filepath = perform_final_training_for_setup(\n",
    "            setup_id=setup_id,\n",
    "            max_epochs=FINAL_TRAIN_EPOCHS,\n",
    "            patience=PATIENCE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            device=device\n",
    "            # The function saves its outputs to files within config.FINAL_TRAINING_DIR\n",
    "        )\n",
    "\n",
    "        final_train_end_time = time.time()\n",
    "        print(f\"\\n{'*'*10} Final Training Stage for {setup_id} finished in {(final_train_end_time - final_train_start_time)/60:.2f} minutes {'*'*10}\")\n",
    "\n",
    "        if saved_model_filepath:\n",
    "            print(f\"Best model saved to: {saved_model_filepath}\")\n",
    "            # This path is also saved internally by the function to results/final_training/best_model_{setup_id}.pth\n",
    "        else:\n",
    "            print(f\"Final training did not complete successfully or model was not saved for {setup_id}. Cannot proceed to evaluation.\")\n",
    "    else:\n",
    "        print(f\"\\nSkipping Stage 2: Final Model Training for {setup_id} due to HPO failure or no best parameters found.\")\n",
    "        saved_model_filepath = None  # Ensure it's None if HPO failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd65a8",
   "metadata": {},
   "source": [
    "## Stage 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4cc41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for setup_id in SETUP_IDS_TO_RUN:\n",
    "    if saved_model_filepath and os.path.exists(saved_model_filepath):  # Proceed only if final training produced a model\n",
    "        print(f\"\\n{'='*20} STAGE 3: Evaluation for {setup_id} {'='*20}\")\n",
    "        eval_start_time = time.time()\n",
    "\n",
    "        # Call the evaluation function from run_evaluation.py\n",
    "        # It will load the model file itself based on setup_id and config.FINAL_TRAINING_DIR\n",
    "        perform_evaluation_for_setup(\n",
    "            setup_id=setup_id,\n",
    "            device=device\n",
    "            # The function saves its outputs to files within config.EVALUATION_DIR\n",
    "        )\n",
    "\n",
    "        eval_end_time = time.time()\n",
    "        print(f\"\\n{'*'*10} Evaluation Stage for {setup_id} finished in {(eval_end_time - eval_start_time)/60:.2f} minutes {'*'*10}\")\n",
    "        print(f\"\\nEvaluation results (predictions and metrics CSVs) are saved in: {config.EVALUATION_DIR}\")\n",
    "    else:\n",
    "        print(f\"\\nSkipping Stage 3: Evaluation for {setup_id} due to final training failure or model not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
