{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Result Analysis\n",
        "\n",
        "In this notebook, we will analyze the evaluation results of multiple image classification models. We will compare their overall performance, examine confusion matrices, and investigate common misclassifications, especially for challenging or similar classes. This analysis will help us understand the strengths and weaknesses of each model and guide future improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PgjErgO7l_Hj"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBi9Bb-emaG4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Configuration ---\n",
        "MODEL_CONFIGS = [\n",
        "    {\n",
        "        \"name\": \"MobileNetV2 (Head Trained, Augmentation)\",\n",
        "        \"prefix\": \"mobilenet_head_aug\",\n",
        "        \"metrics_file\": \"eval_metrics_mobilenet_head_aug.csv\",\n",
        "        \"predictions_file\": \"predictions_mobilenet_head_aug.csv\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"MobileNetV2 (Mid Trained, No Augmentation)\",\n",
        "        \"prefix\": \"mobilenet_mid_noaug\",\n",
        "        \"metrics_file\": \"eval_metrics_mobilenet_mid_noaug.csv\",\n",
        "        \"predictions_file\": \"predictions_mobilenet_mid_noaug.csv\"\n",
        "    },{\"name\": \"MobileNetV2 (Head Trained, No Augmentation)\",\n",
        "        \"prefix\": \"mobilenet_head_noaug\",\n",
        "        \"metrics_file\": \"eval_metrics_mobilenet_head_noaug.csv\",\n",
        "        \"predictions_file\": \"predictions_mobilenet_head_noaug.csv\"\n",
        "    },{\"name\": \"MobileNetV2 (Mid Trained, Augmentation)\",\n",
        "        \"prefix\": \"mobilenet_mid_aug\",\n",
        "        \"metrics_file\": \"eval_metrics_mobilenet_mid_aug.csv\",\n",
        "        \"predictions_file\": \"predictions_mobilenet_mid_aug.csv\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ResNet50 (Mid Trained, No Augmentation)\",\n",
        "        \"prefix\": \"resnet_mid_noaug\",\n",
        "        \"metrics_file\": \"eval_metrics_resnet_mid_noaug.csv\",\n",
        "        \"predictions_file\": \"predictions_resnet_mid_noaug.csv\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ResNet50 (Head Trained, Augmentation)\",\n",
        "        \"prefix\": \"resnet_head_aug\",\n",
        "        \"metrics_file\": \"eval_metrics_resnet_head_aug.csv\",\n",
        "        \"predictions_file\": \"predictions_resnet_head_aug.csv\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ResNet50 (Head Trained, No Augmentation)\",\n",
        "        \"prefix\": \"resnet_head_noaug\",\n",
        "        \"metrics_file\": \"eval_metrics_resnet_head_noaug.csv\",\n",
        "        \"predictions_file\": \"predictions_resnet_head_noaug.csv\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ResNet50 (Mid Trained, Augmentation)\",\n",
        "        \"prefix\": \"resnet_mid_aug\",\n",
        "        \"metrics_file\": \"eval_metrics_resnet_mid_aug.csv\",\n",
        "        \"predictions_file\": \"predictions_resnet_mid_aug.csv\"\n",
        "    },\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "]\n",
        "OUTPUT_DIR = \"analysis_outputs\"\n",
        "N_TOP_CLASSES = 5\n",
        "N_WORST_CLASSES = 5\n",
        "CM_FIG_SIZE = (18, 15) # Adjusted for ~37 classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KtZtozz6m7Wd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Helper Functions ---\n",
        "def load_metrics(filepath):\n",
        "    \"\"\"Loads evaluation metrics, setting class names as index.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        df = df.rename(columns={df.columns[0]: 'class_name'})\n",
        "        df.set_index('class_name', inplace=True)\n",
        "        # Convert metric columns to numeric, coercing errors\n",
        "        for col in ['precision', 'recall', 'f1-score', 'support']:\n",
        "            if col in df.columns:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: Metrics file not found: {filepath}\")\n",
        "        return None\n",
        "\n",
        "def load_predictions(filepath):\n",
        "    \"\"\"Loads prediction details.\"\"\"\n",
        "    try:\n",
        "        return pd.read_csv(filepath)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: Predictions file not found: {filepath}\")\n",
        "        return None\n",
        "\n",
        "def get_class_names(metrics_df):\n",
        "    \"\"\"Extracts class names, excluding summary rows.\"\"\"\n",
        "    if metrics_df is None:\n",
        "        return []\n",
        "    class_names = metrics_df.index.tolist()\n",
        "    return [name for name in class_names if name not in ['macro avg', 'weighted avg']]\n",
        "\n",
        "def plot_confusion_matrix_custom(y_true_idx, y_pred_idx, display_labels, model_name, prefix):\n",
        "    \"\"\"Generates and saves a confusion matrix plot.\"\"\"\n",
        "    if not os.path.exists(OUTPUT_DIR):\n",
        "        os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "    cm = confusion_matrix(y_true_idx, y_pred_idx, labels=np.arange(len(display_labels)))\n",
        "\n",
        "    plt.figure(figsize=CM_FIG_SIZE)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
        "    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', values_format='d')\n",
        "    plt.title(f\"Confusion Matrix: {model_name}\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f\"confusion_matrix_{prefix}.png\"), dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Saved confusion matrix for {model_name} to {OUTPUT_DIR}/confusion_matrix_{prefix}.png\")\n",
        "\n",
        "def analyze_specific_confusion(predictions_df, class1_name, class2_name):\n",
        "    \"\"\"Analyzes confusion between two specific classes.\"\"\"\n",
        "    if predictions_df is None:\n",
        "        return {}\n",
        "\n",
        "    confusion_counts = {}\n",
        "    # Class1 misclassified as Class2\n",
        "    misclassified_c1_as_c2 = predictions_df[\n",
        "        (predictions_df['true_label_name'] == class1_name) &\n",
        "        (predictions_df['predicted_label_name'] == class2_name)\n",
        "    ].shape[0]\n",
        "    confusion_counts[f\"{class1_name}_as_{class2_name}\"] = misclassified_c1_as_c2\n",
        "\n",
        "    # Class2 misclassified as Class1\n",
        "    misclassified_c2_as_c1 = predictions_df[\n",
        "        (predictions_df['true_label_name'] == class2_name) &\n",
        "        (predictions_df['predicted_label_name'] == class1_name)\n",
        "    ].shape[0]\n",
        "    confusion_counts[f\"{class2_name}_as_{class1_name}\"] = misclassified_c2_as_c1\n",
        "    return confusion_counts\n",
        "\n",
        "def get_common_misclassifications(predictions_df, true_class_name, top_n=3):\n",
        "    \"\"\"Identifies the most common incorrect predictions for a given true class.\"\"\"\n",
        "    if predictions_df is None:\n",
        "        return pd.Series(dtype='int64')\n",
        "\n",
        "    misclassifications = predictions_df[\n",
        "        (predictions_df['true_label_name'] == true_class_name) &\n",
        "        (predictions_df['true_label_name'] != predictions_df['predicted_label_name'])\n",
        "    ]\n",
        "    return misclassifications['predicted_label_name'].value_counts().nlargest(top_n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "8WSkd9Bxm_Vs",
        "outputId": "d563cae6-f7e0-4682-8c19-3c98056a9950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== Analyzing: MobileNetV2 (Head Trained, Augmentation) ====================\n",
            "Warning: Metrics file not found: eval_metrics_mobilenet_head_aug.csv\n",
            "Warning: Predictions file not found: predictions_mobilenet_head_aug.csv\n",
            "Skipping MobileNetV2 (Head Trained, Augmentation) due to missing metrics file.\n",
            "\n",
            "==================== Analyzing: MobileNetV2 (Mid Trained, No Augmentation) ====================\n",
            "Warning: Metrics file not found: eval_metrics_mobilenet_mid_noaug.csv\n",
            "Warning: Predictions file not found: predictions_mobilenet_mid_noaug.csv\n",
            "Skipping MobileNetV2 (Mid Trained, No Augmentation) due to missing metrics file.\n",
            "\n",
            "==================== Analyzing: ResNet50 (Mid Trained, No Augmentation) ====================\n",
            "Warning: Metrics file not found: eval_metrics_resnet_mid_noaug.csv\n",
            "Warning: Predictions file not found: predictions_resnet_mid_noaug.csv\n",
            "Skipping ResNet50 (Mid Trained, No Augmentation) due to missing metrics file.\n",
            "\n",
            "\n",
            "==================== Overall Model Comparison ====================\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"None of ['Model'] are in the columns\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/c1/5mk6kfgn4d148hjqz_gw3p5c0000gn/T/ipykernel_23648/765303905.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Could not analyze APBT vs SBT confusion (predictions_df might be missing).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Overall Model Comparison \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_model_summary_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0msummary_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Macro Avg F1-Score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nNote: Loss/Accuracy plots per epoch require training history data, which is not available in the provided CSVs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/math392/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   6118\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6119\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of ['Model'] are in the columns\""
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Main Analysis ---\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(OUTPUT_DIR):\n",
        "        os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "    all_model_summary_metrics = []\n",
        "\n",
        "    for config in MODEL_CONFIGS:\n",
        "        print(f\"\\n{'='*20} Analyzing: {config['name']} {'='*20}\")\n",
        "        metrics_df = load_metrics(config[\"metrics_file\"])\n",
        "        predictions_df = load_predictions(config[\"predictions_file\"])\n",
        "\n",
        "        if metrics_df is None:\n",
        "            print(f\"Skipping {config['name']} due to missing metrics file.\")\n",
        "            continue\n",
        "\n",
        "        class_names_ordered = get_class_names(metrics_df) # Get class names before dropping summary rows\n",
        "\n",
        "        # Store overall metrics\n",
        "        summary_metrics = {\n",
        "            \"Model\": config[\"name\"],\n",
        "            \"Macro Avg F1-Score\": metrics_df.loc['macro avg', 'f1-score'] if 'macro avg' in metrics_df.index else np.nan,\n",
        "            \"Weighted Avg F1-Score\": metrics_df.loc['weighted avg', 'f1-score'] if 'weighted avg' in metrics_df.index else np.nan,\n",
        "            \"Macro Avg Precision\": metrics_df.loc['macro avg', 'precision'] if 'macro avg' in metrics_df.index else np.nan,\n",
        "            \"Macro Avg Recall\": metrics_df.loc['macro avg', 'recall'] if 'macro avg' in metrics_df.index else np.nan,\n",
        "        }\n",
        "        all_model_summary_metrics.append(summary_metrics)\n",
        "\n",
        "        print(\"\\n--- Overall Performance ---\")\n",
        "        print(f\"Macro Average F1-Score: {summary_metrics['Macro Avg F1-Score']:.4f}\")\n",
        "        print(f\"Weighted Average F1-Score: {summary_metrics['Weighted Avg F1-Score']:.4f}\")\n",
        "\n",
        "        # Filter out summary rows for class-specific analysis\n",
        "        class_metrics_df = metrics_df.drop(['macro avg', 'weighted avg'], errors='ignore')\n",
        "\n",
        "        print(f\"\\n--- Top {N_TOP_CLASSES} Performing Classes (by F1-score) ---\")\n",
        "        top_classes = class_metrics_df['f1-score'].nlargest(N_TOP_CLASSES)\n",
        "        print(top_classes)\n",
        "\n",
        "        print(f\"\\n--- Worst {N_WORST_CLASSES} Performing Classes (by F1-score) ---\")\n",
        "        worst_classes = class_metrics_df['f1-score'].nsmallest(N_WORST_CLASSES)\n",
        "        print(worst_classes)\n",
        "\n",
        "        if predictions_df is not None and class_names_ordered:\n",
        "            # Ensure label indices are present and map correctly\n",
        "            if 'true_label_idx' in predictions_df.columns and 'predicted_label_idx' in predictions_df.columns:\n",
        "                # Create a mapping if prediction_df doesn't have consistent label names for indices\n",
        "                # For this script, we assume the indices in predictions_df correspond to an alphabetical sort of class_names_ordered\n",
        "                # or are already correctly mapped. If not, a more robust mapping is needed.\n",
        "                # For simplicity, let's try to get unique sorted names from predictions.\n",
        "                unique_true_labels = sorted(predictions_df['true_label_name'].unique())\n",
        "                if not all(item in unique_true_labels for item in class_names_ordered) or not all(item in class_names_ordered for item in unique_true_labels):\n",
        "                     print(\"Warning: Class name mismatch between metrics and predictions. Using names from predictions for CM.\")\n",
        "                     cm_display_labels = unique_true_labels\n",
        "                else:\n",
        "                     cm_display_labels = class_names_ordered # Prefer names from metrics for consistency\n",
        "\n",
        "                max_idx = max(predictions_df['true_label_idx'].max(), predictions_df['predicted_label_idx'].max())\n",
        "                if max_idx >= len(cm_display_labels):\n",
        "                    print(f\"Warning: Max label index ({max_idx}) exceeds number of display labels ({len(cm_display_labels)}). CM might be incorrect.\")\n",
        "                    # Fallback or error handling could be added here. For now, proceed with caution.\n",
        "\n",
        "                plot_confusion_matrix_custom(\n",
        "                    predictions_df['true_label_idx'],\n",
        "                    predictions_df['predicted_label_idx'],\n",
        "                    display_labels=cm_display_labels,\n",
        "                    model_name=config['name'],\n",
        "                    prefix=config['prefix']\n",
        "                )\n",
        "            else:\n",
        "                print(\"Skipping Confusion Matrix: 'true_label_idx' or 'predicted_label_idx' not found in predictions.\")\n",
        "\n",
        "\n",
        "            print(\"\\n--- Specific Confusions for Lowest Performing Classes ---\")\n",
        "            for class_name, f1 in worst_classes.items():\n",
        "                print(f\"  {class_name} (F1: {f1:.2f}):\")\n",
        "                common_mispreds = get_common_misclassifications(predictions_df, class_name, top_n=3)\n",
        "                if not common_mispreds.empty:\n",
        "                    for pred_class, count in common_mispreds.items():\n",
        "                        print(f\"    Predicted as {pred_class}: {count} times\")\n",
        "                else:\n",
        "                    print(\"    No specific misclassifications data available or class perfectly recalled among errors.\")\n",
        "\n",
        "            print(\"\\n--- American Pit Bull Terrier vs. Staffordshire Bull Terrier Confusion ---\")\n",
        "            apbt_sbt_confusion = analyze_specific_confusion(predictions_df, \"American Pit Bull Terrier\", \"Staffordshire Bull Terrier\")\n",
        "            if apbt_sbt_confusion:\n",
        "                print(f\"  American Pit Bull Terrier misclassified as Staffordshire Bull Terrier: {apbt_sbt_confusion.get('American Pit Bull Terrier_as_Staffordshire Bull Terrier', 0)} times\")\n",
        "                print(f\"  Staffordshire Bull Terrier misclassified as American Pit Bull Terrier: {apbt_sbt_confusion.get('Staffordshire Bull Terrier_as_American Pit Bull Terrier', 0)} times\")\n",
        "            else:\n",
        "                print(\"  Could not analyze APBT vs SBT confusion (predictions_df might be missing).\")\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" Overall Model Comparison \" + \"=\"*20)\n",
        "    summary_df = pd.DataFrame(all_model_summary_metrics)\n",
        "    summary_df.set_index(\"Model\", inplace=True)\n",
        "    print(summary_df.sort_values(by=\"Macro Avg F1-Score\", ascending=False))\n",
        "\n",
        "    print(\"\\nNote: Loss/Accuracy plots per epoch require training history data, which is not available in the provided CSVs.\")\n",
        "    print(f\"Analysis outputs (like confusion matrices) saved to '{OUTPUT_DIR}' directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkWPXQlNnLDr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "math392",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
